{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\Hp\\\\Desktop\\\\EcoDrive Simulator\\\\src\\\\pages\\\\EducationalInsights.js\",\n  _s = $RefreshSig$();\nimport React, { useState } from 'react';\nimport { Link } from 'react-router-dom';\nimport { ArrowLeft, BookOpen, Brain, Zap, Leaf, Target, TrendingUp, Play, ChevronDown, ChevronUp, Award, Clock, Users, Globe, Car, Gauge, Lightbulb, CheckCircle, ExternalLink, Download, Star } from 'lucide-react';\nimport './EducationalInsights.css';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst EducationalInsights = () => {\n  _s();\n  const [expandedSection, setExpandedSection] = useState(null);\n  const [activeVideo, setActiveVideo] = useState(null);\n  const insights = [{\n    id: 'rl-fundamentals',\n    icon: Brain,\n    title: 'Reinforcement Learning Fundamentals',\n    subtitle: 'Understanding the basics of AI learning',\n    content: 'Reinforcement Learning (RL) is a type of machine learning where an AI agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. Unlike supervised learning, RL doesn\\'t require labeled training data - instead, it learns through trial and error, gradually improving its performance over time.',\n    detailedContent: `\n        <h4>Key Components:</h4>\n        <ul>\n          <li><strong>Agent:</strong> The AI system making decisions</li>\n          <li><strong>Environment:</strong> The world the agent interacts with</li>\n          <li><strong>Actions:</strong> What the agent can do</li>\n          <li><strong>Rewards:</strong> Feedback that guides learning</li>\n          <li><strong>Policy:</strong> The strategy the agent uses to choose actions</li>\n        </ul>\n        \n        <h4>How RL Works in Racing:</h4>\n        <p>In our simulator, the AI agent learns to drive by:</p>\n        <ol>\n          <li>Observing track conditions, speed, and position</li>\n          <li>Taking actions (accelerate, brake, steer)</li>\n          <li>Receiving rewards for good driving (speed, efficiency)</li>\n          <li>Learning from mistakes and improving over time</li>\n        </ol>\n      `,\n    benefits: ['Self-learning capability', 'Adapts to new situations', 'Optimizes for long-term rewards'],\n    videoId: '2pWv7GOvuf0',\n    // Reinforcement Learning Explained\n    duration: '8:40',\n    difficulty: 'Beginner',\n    category: 'Fundamentals'\n  }, {\n    id: 'ppo-algorithm',\n    icon: Zap,\n    title: 'PPO Algorithm Deep Dive',\n    subtitle: 'Proximal Policy Optimization explained',\n    content: 'Proximal Policy Optimization (PPO) is a policy gradient method that uses a clipped objective function to prevent large policy updates, making training more stable. It\\'s particularly effective for continuous control tasks like autonomous driving, where smooth and consistent actions are crucial.',\n    detailedContent: `\n        <h4>PPO Advantages:</h4>\n        <ul>\n          <li><strong>Stability:</strong> Clipped objective prevents destructive updates</li>\n          <li><strong>Sample Efficiency:</strong> Uses data more effectively than older methods</li>\n          <li><strong>Continuous Control:</strong> Perfect for driving tasks</li>\n          <li><strong>Robustness:</strong> Works well across different environments</li>\n        </ul>\n        \n        <h4>PPO in Our Simulator:</h4>\n        <p>Our PPO model learns to:</p>\n        <ul>\n          <li>Optimize throttle and steering inputs</li>\n          <li>Balance speed with energy efficiency</li>\n          <li>Adapt to different track layouts</li>\n          <li>Minimize lap times while conserving energy</li>\n        </ul>\n      `,\n    benefits: ['Stable training', 'Sample efficient', 'Works well with continuous actions'],\n    videoId: '5P7I-xPq8u8',\n    // PPO Algorithm Explained\n    duration: '12:15',\n    difficulty: 'Intermediate',\n    category: 'Algorithms'\n  }, {\n    id: 'sac-algorithm',\n    icon: Target,\n    title: 'SAC Algorithm Explained',\n    subtitle: 'Soft Actor-Critic for exploration',\n    content: 'Soft Actor-Critic (SAC) is an off-policy algorithm that maximizes both expected reward and entropy, encouraging exploration while maintaining good performance. This makes it excellent for discovering novel eco-driving strategies that balance efficiency with performance.',\n    detailedContent: `\n        <h4>SAC Key Features:</h4>\n        <ul>\n          <li><strong>Entropy Regularization:</strong> Encourages exploration</li>\n          <li><strong>Off-Policy Learning:</strong> Can learn from past experiences</li>\n          <li><strong>Continuous Actions:</strong> Handles smooth control inputs</li>\n          <li><strong>Sample Efficiency:</strong> Good data utilization</li>\n        </ul>\n        \n        <h4>Why SAC for Eco-Driving:</h4>\n        <p>SAC excels at finding creative solutions:</p>\n        <ul>\n          <li>Discovers unconventional but efficient racing lines</li>\n          <li>Explores different energy management strategies</li>\n          <li>Balances exploration with exploitation</li>\n          <li>Adapts to changing track conditions</li>\n        </ul>\n      `,\n    benefits: ['Encourages exploration', 'Handles continuous actions', 'Balances exploration and exploitation'],\n    videoId: 'bRfUxQs7xJI',\n    // SAC Algorithm Tutorial\n    duration: '15:30',\n    difficulty: 'Intermediate',\n    category: 'Algorithms'\n  }, {\n    id: 'eco-driving',\n    icon: Leaf,\n    title: 'Eco-Driving Principles',\n    subtitle: 'Sustainable racing strategies',\n    content: 'AI-powered eco-driving can reduce fuel consumption by 10-20% while maintaining competitive lap times, contributing to sustainable mobility. The AI learns optimal acceleration patterns, braking strategies, and energy management techniques that human drivers might not naturally adopt.',\n    detailedContent: `\n        <h4>Eco-Driving Techniques:</h4>\n        <ul>\n          <li><strong>Smooth Acceleration:</strong> Gradual speed increases</li>\n          <li><strong>Anticipatory Braking:</strong> Early, gentle braking</li>\n          <li><strong>Optimal Racing Lines:</strong> Minimize distance and energy</li>\n          <li><strong>Energy Management:</strong> Strategic power usage</li>\n        </ul>\n        \n        <h4>Environmental Impact:</h4>\n        <p>Eco-driving benefits:</p>\n        <ul>\n          <li>Reduces CO₂ emissions by 15-25%</li>\n          <li>Decreases fuel consumption significantly</li>\n          <li>Extends vehicle lifespan</li>\n          <li>Promotes sustainable racing practices</li>\n        </ul>\n      `,\n    benefits: ['Reduced fuel consumption', 'Lower CO₂ emissions', 'Maintained performance levels'],\n    videoId: 'Y4M9z4tUMeU',\n    // Eco-Driving Techniques\n    duration: '6:45',\n    difficulty: 'Beginner',\n    category: 'Sustainability'\n  }, {\n    id: 'performance-optimization',\n    icon: TrendingUp,\n    title: 'Performance Optimization',\n    subtitle: 'Continuous learning and adaptation',\n    content: 'Our AI models continuously learn and adapt to different track conditions, weather patterns, and vehicle characteristics. This adaptive learning process ensures optimal performance across various scenarios, making each simulation more efficient than the last.',\n    detailedContent: `\n        <h4>Adaptive Learning Process:</h4>\n        <ul>\n          <li><strong>Track Adaptation:</strong> Learns each circuit's unique characteristics</li>\n          <li><strong>Weather Response:</strong> Adjusts strategy for conditions</li>\n          <li><strong>Vehicle Dynamics:</strong> Optimizes for specific car parameters</li>\n          <li><strong>Continuous Improvement:</strong> Gets better with each lap</li>\n        </ul>\n        \n        <h4>Optimization Metrics:</h4>\n        <ul>\n          <li>Lap time minimization</li>\n          <li>Energy efficiency maximization</li>\n          <li>Consistency across different conditions</li>\n          <li>Adaptability to new scenarios</li>\n        </ul>\n      `,\n    benefits: ['Continuous improvement', 'Adapts to conditions', 'Personalized strategies'],\n    videoId: 'aircAruvnKk',\n    // AI Performance Optimization\n    duration: '10:20',\n    difficulty: 'Advanced',\n    category: 'Optimization'\n  }];\n  const learningPaths = [{\n    title: 'Beginner Path',\n    description: 'Start your RL journey',\n    topics: ['RL Fundamentals', 'Eco-Driving Principles'],\n    duration: '30 min',\n    icon: BookOpen\n  }, {\n    title: 'Intermediate Path',\n    description: 'Dive into algorithms',\n    topics: ['PPO Algorithm', 'SAC Algorithm'],\n    duration: '45 min',\n    icon: Brain\n  }, {\n    title: 'Advanced Path',\n    description: 'Master optimization',\n    topics: ['Performance Optimization', 'Advanced Techniques'],\n    duration: '60 min',\n    icon: Award\n  }];\n  const stats = [{\n    icon: Users,\n    value: '10,000+',\n    label: 'Students Learned'\n  }, {\n    icon: Clock,\n    value: '2.5M',\n    label: 'Hours Saved'\n  }, {\n    icon: Globe,\n    value: '50+',\n    label: 'Countries Reached'\n  }, {\n    icon: Star,\n    value: '4.9/5',\n    label: 'Average Rating'\n  }];\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"educational-insights\",\n    children: /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"container\",\n      children: [/*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"page-header\",\n        children: [/*#__PURE__*/_jsxDEV(Link, {\n          to: \"/\",\n          className: \"back-button\",\n          children: [/*#__PURE__*/_jsxDEV(ArrowLeft, {\n            size: 20\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 221,\n            columnNumber: 13\n          }, this), \"Back to Home\"]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 220,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"h1\", {\n          children: \"Educational Insights\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 224,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n          children: \"Learn about reinforcement learning, AI algorithms, and eco-driving technologies\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 225,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 219,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"insights-content\",\n        children: insights.map((insight, index) => /*#__PURE__*/_jsxDEV(\"div\", {\n          className: \"insight-section\",\n          children: [/*#__PURE__*/_jsxDEV(\"div\", {\n            className: \"insight-header\",\n            children: [/*#__PURE__*/_jsxDEV(\"div\", {\n              className: \"insight-icon\",\n              children: /*#__PURE__*/_jsxDEV(insight.icon, {\n                size: 24\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 233,\n                columnNumber: 19\n              }, this)\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 232,\n              columnNumber: 17\n            }, this), /*#__PURE__*/_jsxDEV(\"h2\", {\n              children: insight.title\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 235,\n              columnNumber: 17\n            }, this)]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 231,\n            columnNumber: 15\n          }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n            children: insight.content\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 237,\n            columnNumber: 15\n          }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n            className: \"benefits-list\",\n            children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n              children: \"Key Benefits:\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 239,\n              columnNumber: 17\n            }, this), /*#__PURE__*/_jsxDEV(\"ul\", {\n              children: insight.benefits.map((benefit, idx) => /*#__PURE__*/_jsxDEV(\"li\", {\n                children: benefit\n              }, idx, false, {\n                fileName: _jsxFileName,\n                lineNumber: 242,\n                columnNumber: 21\n              }, this))\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 240,\n              columnNumber: 17\n            }, this)]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 238,\n            columnNumber: 15\n          }, this)]\n        }, index, true, {\n          fileName: _jsxFileName,\n          lineNumber: 230,\n          columnNumber: 13\n        }, this))\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 228,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 218,\n      columnNumber: 7\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 217,\n    columnNumber: 5\n  }, this);\n};\n_s(EducationalInsights, \"WZ2QJLb+crt718JXT0TariTnR58=\");\n_c = EducationalInsights;\nexport default EducationalInsights;\nvar _c;\n$RefreshReg$(_c, \"EducationalInsights\");","map":{"version":3,"names":["React","useState","Link","ArrowLeft","BookOpen","Brain","Zap","Leaf","Target","TrendingUp","Play","ChevronDown","ChevronUp","Award","Clock","Users","Globe","Car","Gauge","Lightbulb","CheckCircle","ExternalLink","Download","Star","jsxDEV","_jsxDEV","EducationalInsights","_s","expandedSection","setExpandedSection","activeVideo","setActiveVideo","insights","id","icon","title","subtitle","content","detailedContent","benefits","videoId","duration","difficulty","category","learningPaths","description","topics","stats","value","label","className","children","to","size","fileName","_jsxFileName","lineNumber","columnNumber","map","insight","index","benefit","idx","_c","$RefreshReg$"],"sources":["C:/Users/Hp/Desktop/EcoDrive Simulator/src/pages/EducationalInsights.js"],"sourcesContent":["import React, { useState } from 'react';\r\nimport { Link } from 'react-router-dom';\r\nimport { \r\n  ArrowLeft, \r\n  BookOpen, \r\n  Brain, \r\n  Zap, \r\n  Leaf, \r\n  Target, \r\n  TrendingUp, \r\n  Play, \r\n  ChevronDown, \r\n  ChevronUp,\r\n  Award,\r\n  Clock,\r\n  Users,\r\n  Globe,\r\n  Car,\r\n  Gauge,\r\n  Lightbulb,\r\n  CheckCircle,\r\n  ExternalLink,\r\n  Download,\r\n  Star\r\n} from 'lucide-react';\r\nimport './EducationalInsights.css';\r\n\r\nconst EducationalInsights = () => {\r\n  const [expandedSection, setExpandedSection] = useState(null);\r\n  const [activeVideo, setActiveVideo] = useState(null);\r\n\r\n  const insights = [\r\n    {\r\n      id: 'rl-fundamentals',\r\n      icon: Brain,\r\n      title: 'Reinforcement Learning Fundamentals',\r\n      subtitle: 'Understanding the basics of AI learning',\r\n      content: 'Reinforcement Learning (RL) is a type of machine learning where an AI agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. Unlike supervised learning, RL doesn\\'t require labeled training data - instead, it learns through trial and error, gradually improving its performance over time.',\r\n      detailedContent: `\r\n        <h4>Key Components:</h4>\r\n        <ul>\r\n          <li><strong>Agent:</strong> The AI system making decisions</li>\r\n          <li><strong>Environment:</strong> The world the agent interacts with</li>\r\n          <li><strong>Actions:</strong> What the agent can do</li>\r\n          <li><strong>Rewards:</strong> Feedback that guides learning</li>\r\n          <li><strong>Policy:</strong> The strategy the agent uses to choose actions</li>\r\n        </ul>\r\n        \r\n        <h4>How RL Works in Racing:</h4>\r\n        <p>In our simulator, the AI agent learns to drive by:</p>\r\n        <ol>\r\n          <li>Observing track conditions, speed, and position</li>\r\n          <li>Taking actions (accelerate, brake, steer)</li>\r\n          <li>Receiving rewards for good driving (speed, efficiency)</li>\r\n          <li>Learning from mistakes and improving over time</li>\r\n        </ol>\r\n      `,\r\n      benefits: ['Self-learning capability', 'Adapts to new situations', 'Optimizes for long-term rewards'],\r\n      videoId: '2pWv7GOvuf0', // Reinforcement Learning Explained\r\n      duration: '8:40',\r\n      difficulty: 'Beginner',\r\n      category: 'Fundamentals'\r\n    },\r\n    {\r\n      id: 'ppo-algorithm',\r\n      icon: Zap,\r\n      title: 'PPO Algorithm Deep Dive',\r\n      subtitle: 'Proximal Policy Optimization explained',\r\n      content: 'Proximal Policy Optimization (PPO) is a policy gradient method that uses a clipped objective function to prevent large policy updates, making training more stable. It\\'s particularly effective for continuous control tasks like autonomous driving, where smooth and consistent actions are crucial.',\r\n      detailedContent: `\r\n        <h4>PPO Advantages:</h4>\r\n        <ul>\r\n          <li><strong>Stability:</strong> Clipped objective prevents destructive updates</li>\r\n          <li><strong>Sample Efficiency:</strong> Uses data more effectively than older methods</li>\r\n          <li><strong>Continuous Control:</strong> Perfect for driving tasks</li>\r\n          <li><strong>Robustness:</strong> Works well across different environments</li>\r\n        </ul>\r\n        \r\n        <h4>PPO in Our Simulator:</h4>\r\n        <p>Our PPO model learns to:</p>\r\n        <ul>\r\n          <li>Optimize throttle and steering inputs</li>\r\n          <li>Balance speed with energy efficiency</li>\r\n          <li>Adapt to different track layouts</li>\r\n          <li>Minimize lap times while conserving energy</li>\r\n        </ul>\r\n      `,\r\n      benefits: ['Stable training', 'Sample efficient', 'Works well with continuous actions'],\r\n      videoId: '5P7I-xPq8u8', // PPO Algorithm Explained\r\n      duration: '12:15',\r\n      difficulty: 'Intermediate',\r\n      category: 'Algorithms'\r\n    },\r\n    {\r\n      id: 'sac-algorithm',\r\n      icon: Target,\r\n      title: 'SAC Algorithm Explained',\r\n      subtitle: 'Soft Actor-Critic for exploration',\r\n      content: 'Soft Actor-Critic (SAC) is an off-policy algorithm that maximizes both expected reward and entropy, encouraging exploration while maintaining good performance. This makes it excellent for discovering novel eco-driving strategies that balance efficiency with performance.',\r\n      detailedContent: `\r\n        <h4>SAC Key Features:</h4>\r\n        <ul>\r\n          <li><strong>Entropy Regularization:</strong> Encourages exploration</li>\r\n          <li><strong>Off-Policy Learning:</strong> Can learn from past experiences</li>\r\n          <li><strong>Continuous Actions:</strong> Handles smooth control inputs</li>\r\n          <li><strong>Sample Efficiency:</strong> Good data utilization</li>\r\n        </ul>\r\n        \r\n        <h4>Why SAC for Eco-Driving:</h4>\r\n        <p>SAC excels at finding creative solutions:</p>\r\n        <ul>\r\n          <li>Discovers unconventional but efficient racing lines</li>\r\n          <li>Explores different energy management strategies</li>\r\n          <li>Balances exploration with exploitation</li>\r\n          <li>Adapts to changing track conditions</li>\r\n        </ul>\r\n      `,\r\n      benefits: ['Encourages exploration', 'Handles continuous actions', 'Balances exploration and exploitation'],\r\n      videoId: 'bRfUxQs7xJI', // SAC Algorithm Tutorial\r\n      duration: '15:30',\r\n      difficulty: 'Intermediate',\r\n      category: 'Algorithms'\r\n    },\r\n    {\r\n      id: 'eco-driving',\r\n      icon: Leaf,\r\n      title: 'Eco-Driving Principles',\r\n      subtitle: 'Sustainable racing strategies',\r\n      content: 'AI-powered eco-driving can reduce fuel consumption by 10-20% while maintaining competitive lap times, contributing to sustainable mobility. The AI learns optimal acceleration patterns, braking strategies, and energy management techniques that human drivers might not naturally adopt.',\r\n      detailedContent: `\r\n        <h4>Eco-Driving Techniques:</h4>\r\n        <ul>\r\n          <li><strong>Smooth Acceleration:</strong> Gradual speed increases</li>\r\n          <li><strong>Anticipatory Braking:</strong> Early, gentle braking</li>\r\n          <li><strong>Optimal Racing Lines:</strong> Minimize distance and energy</li>\r\n          <li><strong>Energy Management:</strong> Strategic power usage</li>\r\n        </ul>\r\n        \r\n        <h4>Environmental Impact:</h4>\r\n        <p>Eco-driving benefits:</p>\r\n        <ul>\r\n          <li>Reduces CO₂ emissions by 15-25%</li>\r\n          <li>Decreases fuel consumption significantly</li>\r\n          <li>Extends vehicle lifespan</li>\r\n          <li>Promotes sustainable racing practices</li>\r\n        </ul>\r\n      `,\r\n      benefits: ['Reduced fuel consumption', 'Lower CO₂ emissions', 'Maintained performance levels'],\r\n      videoId: 'Y4M9z4tUMeU', // Eco-Driving Techniques\r\n      duration: '6:45',\r\n      difficulty: 'Beginner',\r\n      category: 'Sustainability'\r\n    },\r\n    {\r\n      id: 'performance-optimization',\r\n      icon: TrendingUp,\r\n      title: 'Performance Optimization',\r\n      subtitle: 'Continuous learning and adaptation',\r\n      content: 'Our AI models continuously learn and adapt to different track conditions, weather patterns, and vehicle characteristics. This adaptive learning process ensures optimal performance across various scenarios, making each simulation more efficient than the last.',\r\n      detailedContent: `\r\n        <h4>Adaptive Learning Process:</h4>\r\n        <ul>\r\n          <li><strong>Track Adaptation:</strong> Learns each circuit's unique characteristics</li>\r\n          <li><strong>Weather Response:</strong> Adjusts strategy for conditions</li>\r\n          <li><strong>Vehicle Dynamics:</strong> Optimizes for specific car parameters</li>\r\n          <li><strong>Continuous Improvement:</strong> Gets better with each lap</li>\r\n        </ul>\r\n        \r\n        <h4>Optimization Metrics:</h4>\r\n        <ul>\r\n          <li>Lap time minimization</li>\r\n          <li>Energy efficiency maximization</li>\r\n          <li>Consistency across different conditions</li>\r\n          <li>Adaptability to new scenarios</li>\r\n        </ul>\r\n      `,\r\n      benefits: ['Continuous improvement', 'Adapts to conditions', 'Personalized strategies'],\r\n      videoId: 'aircAruvnKk', // AI Performance Optimization\r\n      duration: '10:20',\r\n      difficulty: 'Advanced',\r\n      category: 'Optimization'\r\n    }\r\n  ];\r\n\r\n  const learningPaths = [\r\n    {\r\n      title: 'Beginner Path',\r\n      description: 'Start your RL journey',\r\n      topics: ['RL Fundamentals', 'Eco-Driving Principles'],\r\n      duration: '30 min',\r\n      icon: BookOpen\r\n    },\r\n    {\r\n      title: 'Intermediate Path',\r\n      description: 'Dive into algorithms',\r\n      topics: ['PPO Algorithm', 'SAC Algorithm'],\r\n      duration: '45 min',\r\n      icon: Brain\r\n    },\r\n    {\r\n      title: 'Advanced Path',\r\n      description: 'Master optimization',\r\n      topics: ['Performance Optimization', 'Advanced Techniques'],\r\n      duration: '60 min',\r\n      icon: Award\r\n    }\r\n  ];\r\n\r\n  const stats = [\r\n    { icon: Users, value: '10,000+', label: 'Students Learned' },\r\n    { icon: Clock, value: '2.5M', label: 'Hours Saved' },\r\n    { icon: Globe, value: '50+', label: 'Countries Reached' },\r\n    { icon: Star, value: '4.9/5', label: 'Average Rating' }\r\n  ];\r\n\r\n  return (\r\n    <div className=\"educational-insights\">\r\n      <div className=\"container\">\r\n        <div className=\"page-header\">\r\n          <Link to=\"/\" className=\"back-button\">\r\n            <ArrowLeft size={20} />\r\n            Back to Home\r\n          </Link>\r\n          <h1>Educational Insights</h1>\r\n          <p>Learn about reinforcement learning, AI algorithms, and eco-driving technologies</p>\r\n        </div>\r\n\r\n        <div className=\"insights-content\">\r\n          {insights.map((insight, index) => (\r\n            <div key={index} className=\"insight-section\">\r\n              <div className=\"insight-header\">\r\n                <div className=\"insight-icon\">\r\n                  <insight.icon size={24} />\r\n                </div>\r\n                <h2>{insight.title}</h2>\r\n              </div>\r\n              <p>{insight.content}</p>\r\n              <div className=\"benefits-list\">\r\n                <h4>Key Benefits:</h4>\r\n                <ul>\r\n                  {insight.benefits.map((benefit, idx) => (\r\n                    <li key={idx}>{benefit}</li>\r\n                  ))}\r\n                </ul>\r\n              </div>\r\n            </div>\r\n          ))}\r\n        </div>\r\n      </div>\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default EducationalInsights;"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,QAAQ,QAAQ,OAAO;AACvC,SAASC,IAAI,QAAQ,kBAAkB;AACvC,SACEC,SAAS,EACTC,QAAQ,EACRC,KAAK,EACLC,GAAG,EACHC,IAAI,EACJC,MAAM,EACNC,UAAU,EACVC,IAAI,EACJC,WAAW,EACXC,SAAS,EACTC,KAAK,EACLC,KAAK,EACLC,KAAK,EACLC,KAAK,EACLC,GAAG,EACHC,KAAK,EACLC,SAAS,EACTC,WAAW,EACXC,YAAY,EACZC,QAAQ,EACRC,IAAI,QACC,cAAc;AACrB,OAAO,2BAA2B;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAEnC,MAAMC,mBAAmB,GAAGA,CAAA,KAAM;EAAAC,EAAA;EAChC,MAAM,CAACC,eAAe,EAAEC,kBAAkB,CAAC,GAAG5B,QAAQ,CAAC,IAAI,CAAC;EAC5D,MAAM,CAAC6B,WAAW,EAAEC,cAAc,CAAC,GAAG9B,QAAQ,CAAC,IAAI,CAAC;EAEpD,MAAM+B,QAAQ,GAAG,CACf;IACEC,EAAE,EAAE,iBAAiB;IACrBC,IAAI,EAAE7B,KAAK;IACX8B,KAAK,EAAE,qCAAqC;IAC5CC,QAAQ,EAAE,yCAAyC;IACnDC,OAAO,EAAE,gWAAgW;IACzWC,eAAe,EAAE;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;IACDC,QAAQ,EAAE,CAAC,0BAA0B,EAAE,0BAA0B,EAAE,iCAAiC,CAAC;IACrGC,OAAO,EAAE,aAAa;IAAE;IACxBC,QAAQ,EAAE,MAAM;IAChBC,UAAU,EAAE,UAAU;IACtBC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,eAAe;IACnBC,IAAI,EAAE5B,GAAG;IACT6B,KAAK,EAAE,yBAAyB;IAChCC,QAAQ,EAAE,wCAAwC;IAClDC,OAAO,EAAE,ySAAyS;IAClTC,eAAe,EAAE;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;IACDC,QAAQ,EAAE,CAAC,iBAAiB,EAAE,kBAAkB,EAAE,oCAAoC,CAAC;IACvFC,OAAO,EAAE,aAAa;IAAE;IACxBC,QAAQ,EAAE,OAAO;IACjBC,UAAU,EAAE,cAAc;IAC1BC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,eAAe;IACnBC,IAAI,EAAE1B,MAAM;IACZ2B,KAAK,EAAE,yBAAyB;IAChCC,QAAQ,EAAE,mCAAmC;IAC7CC,OAAO,EAAE,gRAAgR;IACzRC,eAAe,EAAE;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;IACDC,QAAQ,EAAE,CAAC,wBAAwB,EAAE,4BAA4B,EAAE,uCAAuC,CAAC;IAC3GC,OAAO,EAAE,aAAa;IAAE;IACxBC,QAAQ,EAAE,OAAO;IACjBC,UAAU,EAAE,cAAc;IAC1BC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,aAAa;IACjBC,IAAI,EAAE3B,IAAI;IACV4B,KAAK,EAAE,wBAAwB;IAC/BC,QAAQ,EAAE,+BAA+B;IACzCC,OAAO,EAAE,6RAA6R;IACtSC,eAAe,EAAE;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;IACDC,QAAQ,EAAE,CAAC,0BAA0B,EAAE,qBAAqB,EAAE,+BAA+B,CAAC;IAC9FC,OAAO,EAAE,aAAa;IAAE;IACxBC,QAAQ,EAAE,MAAM;IAChBC,UAAU,EAAE,UAAU;IACtBC,QAAQ,EAAE;EACZ,CAAC,EACD;IACEV,EAAE,EAAE,0BAA0B;IAC9BC,IAAI,EAAEzB,UAAU;IAChB0B,KAAK,EAAE,0BAA0B;IACjCC,QAAQ,EAAE,oCAAoC;IAC9CC,OAAO,EAAE,oQAAoQ;IAC7QC,eAAe,EAAE;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;IACDC,QAAQ,EAAE,CAAC,wBAAwB,EAAE,sBAAsB,EAAE,yBAAyB,CAAC;IACvFC,OAAO,EAAE,aAAa;IAAE;IACxBC,QAAQ,EAAE,OAAO;IACjBC,UAAU,EAAE,UAAU;IACtBC,QAAQ,EAAE;EACZ,CAAC,CACF;EAED,MAAMC,aAAa,GAAG,CACpB;IACET,KAAK,EAAE,eAAe;IACtBU,WAAW,EAAE,uBAAuB;IACpCC,MAAM,EAAE,CAAC,iBAAiB,EAAE,wBAAwB,CAAC;IACrDL,QAAQ,EAAE,QAAQ;IAClBP,IAAI,EAAE9B;EACR,CAAC,EACD;IACE+B,KAAK,EAAE,mBAAmB;IAC1BU,WAAW,EAAE,sBAAsB;IACnCC,MAAM,EAAE,CAAC,eAAe,EAAE,eAAe,CAAC;IAC1CL,QAAQ,EAAE,QAAQ;IAClBP,IAAI,EAAE7B;EACR,CAAC,EACD;IACE8B,KAAK,EAAE,eAAe;IACtBU,WAAW,EAAE,qBAAqB;IAClCC,MAAM,EAAE,CAAC,0BAA0B,EAAE,qBAAqB,CAAC;IAC3DL,QAAQ,EAAE,QAAQ;IAClBP,IAAI,EAAErB;EACR,CAAC,CACF;EAED,MAAMkC,KAAK,GAAG,CACZ;IAAEb,IAAI,EAAEnB,KAAK;IAAEiC,KAAK,EAAE,SAAS;IAAEC,KAAK,EAAE;EAAmB,CAAC,EAC5D;IAAEf,IAAI,EAAEpB,KAAK;IAAEkC,KAAK,EAAE,MAAM;IAAEC,KAAK,EAAE;EAAc,CAAC,EACpD;IAAEf,IAAI,EAAElB,KAAK;IAAEgC,KAAK,EAAE,KAAK;IAAEC,KAAK,EAAE;EAAoB,CAAC,EACzD;IAAEf,IAAI,EAAEX,IAAI;IAAEyB,KAAK,EAAE,OAAO;IAAEC,KAAK,EAAE;EAAiB,CAAC,CACxD;EAED,oBACExB,OAAA;IAAKyB,SAAS,EAAC,sBAAsB;IAAAC,QAAA,eACnC1B,OAAA;MAAKyB,SAAS,EAAC,WAAW;MAAAC,QAAA,gBACxB1B,OAAA;QAAKyB,SAAS,EAAC,aAAa;QAAAC,QAAA,gBAC1B1B,OAAA,CAACvB,IAAI;UAACkD,EAAE,EAAC,GAAG;UAACF,SAAS,EAAC,aAAa;UAAAC,QAAA,gBAClC1B,OAAA,CAACtB,SAAS;YAACkD,IAAI,EAAE;UAAG;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAE,CAAC,gBAEzB;QAAA;UAAAH,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAM,CAAC,eACPhC,OAAA;UAAA0B,QAAA,EAAI;QAAoB;UAAAG,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC,eAC7BhC,OAAA;UAAA0B,QAAA,EAAG;QAA+E;UAAAG,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAG,CAAC;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OACnF,CAAC,eAENhC,OAAA;QAAKyB,SAAS,EAAC,kBAAkB;QAAAC,QAAA,EAC9BnB,QAAQ,CAAC0B,GAAG,CAAC,CAACC,OAAO,EAAEC,KAAK,kBAC3BnC,OAAA;UAAiByB,SAAS,EAAC,iBAAiB;UAAAC,QAAA,gBAC1C1B,OAAA;YAAKyB,SAAS,EAAC,gBAAgB;YAAAC,QAAA,gBAC7B1B,OAAA;cAAKyB,SAAS,EAAC,cAAc;cAAAC,QAAA,eAC3B1B,OAAA,CAACkC,OAAO,CAACzB,IAAI;gBAACmB,IAAI,EAAE;cAAG;gBAAAC,QAAA,EAAAC,YAAA;gBAAAC,UAAA;gBAAAC,YAAA;cAAA,OAAE;YAAC;cAAAH,QAAA,EAAAC,YAAA;cAAAC,UAAA;cAAAC,YAAA;YAAA,OACvB,CAAC,eACNhC,OAAA;cAAA0B,QAAA,EAAKQ,OAAO,CAACxB;YAAK;cAAAmB,QAAA,EAAAC,YAAA;cAAAC,UAAA;cAAAC,YAAA;YAAA,OAAK,CAAC;UAAA;YAAAH,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OACrB,CAAC,eACNhC,OAAA;YAAA0B,QAAA,EAAIQ,OAAO,CAACtB;UAAO;YAAAiB,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI,CAAC,eACxBhC,OAAA;YAAKyB,SAAS,EAAC,eAAe;YAAAC,QAAA,gBAC5B1B,OAAA;cAAA0B,QAAA,EAAI;YAAa;cAAAG,QAAA,EAAAC,YAAA;cAAAC,UAAA;cAAAC,YAAA;YAAA,OAAI,CAAC,eACtBhC,OAAA;cAAA0B,QAAA,EACGQ,OAAO,CAACpB,QAAQ,CAACmB,GAAG,CAAC,CAACG,OAAO,EAAEC,GAAG,kBACjCrC,OAAA;gBAAA0B,QAAA,EAAeU;cAAO,GAAbC,GAAG;gBAAAR,QAAA,EAAAC,YAAA;gBAAAC,UAAA;gBAAAC,YAAA;cAAA,OAAe,CAC5B;YAAC;cAAAH,QAAA,EAAAC,YAAA;cAAAC,UAAA;cAAAC,YAAA;YAAA,OACA,CAAC;UAAA;YAAAH,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OACF,CAAC;QAAA,GAfEG,KAAK;UAAAN,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAgBV,CACN;MAAC;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OACC,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACH;EAAC;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACH,CAAC;AAEV,CAAC;AAAC9B,EAAA,CAhOID,mBAAmB;AAAAqC,EAAA,GAAnBrC,mBAAmB;AAkOzB,eAAeA,mBAAmB;AAAC,IAAAqC,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}